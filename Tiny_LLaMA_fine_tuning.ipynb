{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from peft import get_peft_model, PeftConfig, PeftModel, LoraConfig, prepare_model_for_kbit_training\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, TrainingArguments, GenerationConfig\n",
    "from trl import SFTTrainer\n",
    "from datasets import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Package                 Version\n",
      "----------------------- ------------\n",
      "absl-py                 2.1.0\n",
      "accelerate              0.27.2\n",
      "aiohttp                 3.9.5\n",
      "aiosignal               1.3.1\n",
      "asttokens               2.4.1\n",
      "attrs                   23.2.0\n",
      "bitsandbytes            0.43.1\n",
      "certifi                 2024.2.2\n",
      "charset-normalizer      3.3.2\n",
      "colorama                0.4.6\n",
      "comm                    0.2.2\n",
      "datasets                2.19.0\n",
      "debugpy                 1.8.1\n",
      "decorator               5.1.1\n",
      "dill                    0.3.8\n",
      "executing               2.0.1\n",
      "filelock                3.13.1\n",
      "frozenlist              1.4.1\n",
      "fsspec                  2024.2.0\n",
      "grpcio                  1.62.2\n",
      "huggingface-hub         0.22.2\n",
      "idna                    3.7\n",
      "intel-openmp            2021.4.0\n",
      "ipykernel               6.29.4\n",
      "ipython                 8.24.0\n",
      "ipywidgets              8.1.2\n",
      "jedi                    0.19.1\n",
      "Jinja2                  3.1.3\n",
      "jupyter_client          8.6.1\n",
      "jupyter_core            5.7.2\n",
      "jupyterlab_widgets      3.0.10\n",
      "Markdown                3.6\n",
      "MarkupSafe              2.1.5\n",
      "matplotlib-inline       0.1.7\n",
      "mkl                     2021.4.0\n",
      "mpmath                  1.3.0\n",
      "multidict               6.0.5\n",
      "multiprocess            0.70.16\n",
      "nest-asyncio            1.6.0\n",
      "networkx                3.2.1\n",
      "numpy                   1.26.3\n",
      "packaging               24.0\n",
      "pandas                  2.2.2\n",
      "parso                   0.8.4\n",
      "peft                    0.4.0\n",
      "pillow                  10.2.0\n",
      "pip                     24.0\n",
      "platformdirs            4.2.1\n",
      "prompt-toolkit          3.0.43\n",
      "protobuf                5.26.1\n",
      "psutil                  5.9.8\n",
      "pure-eval               0.2.2\n",
      "pyarrow                 16.0.0\n",
      "pyarrow-hotfix          0.6\n",
      "Pygments                2.17.2\n",
      "python-dateutil         2.9.0.post0\n",
      "pytz                    2024.1\n",
      "pywin32                 306\n",
      "PyYAML                  6.0.1\n",
      "pyzmq                   26.0.2\n",
      "regex                   2024.4.16\n",
      "requests                2.31.0\n",
      "safetensors             0.4.3\n",
      "setuptools              69.5.1\n",
      "six                     1.16.0\n",
      "stack-data              0.6.3\n",
      "sympy                   1.12\n",
      "tbb                     2021.11.0\n",
      "tensorboard             2.16.2\n",
      "tensorboard-data-server 0.7.2\n",
      "tokenizers              0.15.2\n",
      "torch                   2.3.0+cu121\n",
      "torchaudio              2.3.0+cu121\n",
      "torchvision             0.18.0+cu121\n",
      "tornado                 6.4\n",
      "tqdm                    4.66.2\n",
      "traitlets               5.14.3\n",
      "transformers            4.38.2\n",
      "trl                     0.4.7\n",
      "typing_extensions       4.9.0\n",
      "tzdata                  2024.1\n",
      "urllib3                 2.2.1\n",
      "wcwidth                 0.2.13\n",
      "Werkzeug                3.0.2\n",
      "widgetsnbextension      4.0.10\n",
      "xxhash                  3.4.1\n",
      "yarl                    1.9.4\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "#installed libraries (venv used exclusively for this notebook)\n",
    "%pip list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Song</th>\n",
       "      <th>Author_band</th>\n",
       "      <th>Songwriters</th>\n",
       "      <th>Duration</th>\n",
       "      <th>Album</th>\n",
       "      <th>Album_type</th>\n",
       "      <th>No_on_album</th>\n",
       "      <th>Release_date</th>\n",
       "      <th>Link</th>\n",
       "      <th>Lyrics</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2 x 4</td>\n",
       "      <td>Metallica</td>\n",
       "      <td>James Hetfield,Lars Ulrich,Kirk Hammett</td>\n",
       "      <td>5:28</td>\n",
       "      <td>Load</td>\n",
       "      <td>Album</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1996-06-04</td>\n",
       "      <td>https://www.metallica.com/songs/2x4.html</td>\n",
       "      <td>I’m gonna make you shake you take you,I’m gonn...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>53rd &amp; 3rd</td>\n",
       "      <td>The Ramones</td>\n",
       "      <td>Douglas Colvin</td>\n",
       "      <td>2:21</td>\n",
       "      <td>We're a Happy Family: A Tribute to Ramones</td>\n",
       "      <td>Compilation</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2003-02-11</td>\n",
       "      <td>https://www.metallica.com/songs/53rd-and-3rd.html</td>\n",
       "      <td>If you think you can well come on man,I was a ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>72 Seasons</td>\n",
       "      <td>Metallica</td>\n",
       "      <td>James Hetfield,Lars Ulrich,Kirk Hammett</td>\n",
       "      <td>7:39</td>\n",
       "      <td>72 Seasons</td>\n",
       "      <td>Album</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2023-04-14</td>\n",
       "      <td>https://www.metallica.com/songs/72-seasons.html</td>\n",
       "      <td>Feeding on the wrath of man,Shot down,Traumati...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Ain’t My Bitch</td>\n",
       "      <td>Metallica</td>\n",
       "      <td>James Hetfield,Lars Ulrich</td>\n",
       "      <td>5:04</td>\n",
       "      <td>Load</td>\n",
       "      <td>Album</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1996-06-04</td>\n",
       "      <td>https://www.metallica.com/songs/aint-my-bitch....</td>\n",
       "      <td>Outta my way,Outta my day,Out of your mind and...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>All Day and All of the Night</td>\n",
       "      <td>The Kinks</td>\n",
       "      <td>Ray Davies</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://www.metallica.com/songs/all-day-and-al...</td>\n",
       "      <td>I’m not content to be with you in the daytime,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           Song  Author_band  \\\n",
       "0                         2 x 4    Metallica   \n",
       "1                    53rd & 3rd  The Ramones   \n",
       "2                    72 Seasons    Metallica   \n",
       "3                Ain’t My Bitch    Metallica   \n",
       "4  All Day and All of the Night    The Kinks   \n",
       "\n",
       "                               Songwriters Duration  \\\n",
       "0  James Hetfield,Lars Ulrich,Kirk Hammett     5:28   \n",
       "1                           Douglas Colvin     2:21   \n",
       "2  James Hetfield,Lars Ulrich,Kirk Hammett     7:39   \n",
       "3               James Hetfield,Lars Ulrich     5:04   \n",
       "4                               Ray Davies      NaN   \n",
       "\n",
       "                                        Album   Album_type  No_on_album  \\\n",
       "0                                        Load        Album          2.0   \n",
       "1  We're a Happy Family: A Tribute to Ramones  Compilation          4.0   \n",
       "2                                  72 Seasons        Album          1.0   \n",
       "3                                        Load        Album          1.0   \n",
       "4                                         NaN          NaN          NaN   \n",
       "\n",
       "  Release_date                                               Link  \\\n",
       "0   1996-06-04           https://www.metallica.com/songs/2x4.html   \n",
       "1   2003-02-11  https://www.metallica.com/songs/53rd-and-3rd.html   \n",
       "2   2023-04-14    https://www.metallica.com/songs/72-seasons.html   \n",
       "3   1996-06-04  https://www.metallica.com/songs/aint-my-bitch....   \n",
       "4          NaN  https://www.metallica.com/songs/all-day-and-al...   \n",
       "\n",
       "                                              Lyrics  \n",
       "0  I’m gonna make you shake you take you,I’m gonn...  \n",
       "1  If you think you can well come on man,I was a ...  \n",
       "2  Feeding on the wrath of man,Shot down,Traumati...  \n",
       "3  Outta my way,Outta my day,Out of your mind and...  \n",
       "4  I’m not content to be with you in the daytime,...  "
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Dataset\n",
    "df = pd.read_csv('Metallica_songs.csv', sep=';')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I’m gonna make you shake you take you\n",
      "I’m gonna be the one who breaks you\n",
      "Put the screws to you yeah my way\n",
      "Yeah come on and come on come and make my day\n",
      "Make my day\n",
      " \n",
      "Got some hell to pay I steal you\n"
     ]
    }
   ],
   "source": [
    "#Checking lyrics\n",
    "lyrics = '\\n'.join(df.loc[:,'Lyrics']).replace(\",\", \"\\n\")\n",
    "print(lyrics[:200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t \n",
      "   ! & ' ( ) - . 1 2 3 4 5 6 7 8 9 : ? A B C D E F G H I J K L M N O P Q R S T U V W X Y Z a b c d e f g h i j k l m n o p q r s t u v w x y z   Æ é – ‘ ’ “ ” …\n"
     ]
    }
   ],
   "source": [
    "#All characters\n",
    "print(' '.join(sorted(set(lyrics))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " !'.123456789ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n"
     ]
    }
   ],
   "source": [
    "#Cleaning special characters\n",
    "cleaned_lyrics = re.sub(r\"[^a-zA-Z0-9 ,'.!\\n]\", '', lyrics)\n",
    "print(''.join(sorted(set(cleaned_lyrics))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "448\n"
     ]
    }
   ],
   "source": [
    "#Splitting\n",
    "split_point = int(len(cleaned_lyrics)*0.95)\n",
    "train_data = cleaned_lyrics[:split_point]\n",
    "test_data = cleaned_lyrics[split_point:]\n",
    "train_data_seg = []\n",
    "for i in range(0, len(train_data), 500):\n",
    "        text = train_data[i:min(i+500, len(train_data))]\n",
    "        train_data_seg.append(text)\n",
    "train_data_seg = Dataset.from_dict({'text':train_data_seg})\n",
    "print(len(train_data_seg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f08c6b7eeb994e8dbc120d3967750c59",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Log in to HugFace\n",
    "from huggingface_hub import notebook_login\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4882dbb0ca34efdb56dd108cf114273",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/607 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\bbnfa\\Python_bgdn\\LLaMA\\.venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:148: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\bbnfa\\.cache\\huggingface\\hub\\models--PY007--TinyLlama-1.1B-step-50K-105b. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cbd32dad7f9e449985c8f18ebcdb8f6d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/4.40G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c811a9c2da24ccd8a10db60b5cadb46",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/129 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Pulling model\n",
    "model_name = \"PY007/TinyLlama-1.1B-step-50K-105b\"\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,           \n",
    "    bnb_4bit_quant_type=\"nf4\",    \n",
    "    bnb_4bit_use_double_quant=True, \n",
    "    bnb_4bit_compute_dtype=torch.bfloat16, \n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=bnb_config, \n",
    "    device_map=\"auto\",  \n",
    "    trust_remote_code=True, \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99150e4e2c9446ea81a2a24947d6725d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/776 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cba0ad831b8d4375971fc52540d6696a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "747b81cc2da341f289744b9afaf1d5a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.84M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf8420832d5e4ffb845dd42653b1642c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/414 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True) \n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\bbnfa\\Python_bgdn\\LLaMA\\.venv\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Generating lyrics with the base model\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "def generate_lyrics(query, model):\n",
    "    encoding = tokenizer(query, return_tensors=\"pt\").to(device)\n",
    "    generation_config = GenerationConfig(max_new_tokens=250, pad_token_id = tokenizer.eos_token_id,repetition_penalty=1.3, eos_token_id = tokenizer.eos_token_id)\n",
    "    outputs = model.generate(input_ids=encoding.input_ids, generation_config=generation_config)\n",
    "    text_output = tokenizer.decode(outputs[0],skip_special_tokens=True)    \n",
    "    output = text_output[len(query):]\n",
    "    return output\n",
    "\n",
    "text_output = generate_lyrics(test_data[2100:2600], model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unk or sober I feel like Im dead\n",
      " \n",
      "Its all about money and power\n",
      "And if you dont have your cake and eat it too\n",
      "Youll never know whats going on inside\n",
      " \n",
      "So get up off that rock and run away\n",
      "Get up off that rock and run away\n",
      "From those who would steal your life\n",
      "Now isnt time for that man\n",
      "Too late to save him from his own wrath\n",
      " \n",
      "The world has changed since then\n",
      "He was born into poverty but he grew up rich\n",
      "Into an unjust society\n",
      "A new kind of oppression\n",
      "That no one can escape\n",
      " \n",
      "Were gonna kill them with our guns\n",
      "We gotta make sure nobody gets hurt\n",
      "Just because we are poor doesnt mean we cant fight back\n",
      " \n",
      "Shocking news coming through loud and clear\n",
      "It seems the whole world is against us\n",
      "Everyone wants their piece of the pie\n",
      "All want their share of the loot\n",
      " \n",
      "Theres nothing wrong with being rich\n",
      "No matter how much you earn\n",
      "There is always something wrong with being poor\n",
      "Because everyone else does aswell\n",
      " \n",
      "Poverty is a lie\n",
      "Lies told by thieves\n"
     ]
    }
   ],
   "source": [
    "print(text_output.replace(\",\", \"\\n\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting parameters\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "lora_alpha = 32\n",
    "lora_dropout = 0.05 \n",
    "lora_rank = 32 \n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    lora_alpha=lora_alpha,\n",
    "    lora_dropout=lora_dropout,\n",
    "    r=lora_rank,\n",
    "    bias=\"none\",  \n",
    "    task_type=\"CAUSAL_LM\")\n",
    "\n",
    "peft_model = get_peft_model(model, peft_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting train args\n",
    "output_dir = \"bnalyv/tinyllama\" # Model repo HugFace destination\n",
    "per_device_train_batch_size = 3\n",
    "gradient_accumulation_steps = 2  \n",
    "optim = \"paged_adamw_32bit\" \n",
    "save_strategy=\"steps\" \n",
    "save_steps = 10 \n",
    "logging_steps = 10  \n",
    "learning_rate = 2e-3  \n",
    "max_grad_norm = 0.3 \n",
    "max_steps = 200     \n",
    "warmup_ratio = 0.03 \n",
    "lr_scheduler_type = \"cosine\" \n",
    "\n",
    "training_arguments = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    per_device_train_batch_size=per_device_train_batch_size,\n",
    "    gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "    optim=optim,\n",
    "    save_steps=save_steps,\n",
    "    logging_steps=logging_steps,\n",
    "    learning_rate=learning_rate,\n",
    "    max_grad_norm=max_grad_norm,\n",
    "    max_steps=max_steps,\n",
    "    warmup_ratio=warmup_ratio,\n",
    "    lr_scheduler_type=lr_scheduler_type,\n",
    "    push_to_hub=True,\n",
    "    report_to='none'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3d1a1b118214adea6228370f1458549",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/448 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer = SFTTrainer(\n",
    "    model=peft_model,\n",
    "    train_dataset=train_data_seg,\n",
    "    peft_config=peft_config,\n",
    "    max_seq_length=500,\n",
    "    dataset_text_field='text',\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_arguments\n",
    ")\n",
    "peft_model.config.use_cache = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f07080570ff84dc9a3f58ea4a93e0ae3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Checkpoint destination directory bnalyv/tinyllama\\checkpoint-10 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.1583, 'grad_norm': 0.4162677228450775, 'learning_rate': 0.0019979028262377117, 'epoch': 0.13}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Checkpoint destination directory bnalyv/tinyllama\\checkpoint-20 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.9247, 'grad_norm': 0.31791678071022034, 'learning_rate': 0.001974410524646926, 'epoch': 0.27}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Checkpoint destination directory bnalyv/tinyllama\\checkpoint-30 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.9225, 'grad_norm': 0.318973571062088, 'learning_rate': 0.0019254212296427042, 'epoch': 0.4}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Checkpoint destination directory bnalyv/tinyllama\\checkpoint-40 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.9696, 'grad_norm': 0.3652134835720062, 'learning_rate': 0.0018522168236559692, 'epoch': 0.53}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Checkpoint destination directory bnalyv/tinyllama\\checkpoint-50 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.9225, 'grad_norm': 0.32852327823638916, 'learning_rate': 0.0017567128158176952, 'epoch': 0.67}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Checkpoint destination directory bnalyv/tinyllama\\checkpoint-60 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.8776, 'grad_norm': 0.3380332291126251, 'learning_rate': 0.00164140821963114, 'epoch': 0.8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Checkpoint destination directory bnalyv/tinyllama\\checkpoint-70 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.9605, 'grad_norm': 0.3779241144657135, 'learning_rate': 0.001509320162328763, 'epoch': 0.93}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Checkpoint destination directory bnalyv/tinyllama\\checkpoint-80 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.7922, 'grad_norm': 0.3565402626991272, 'learning_rate': 0.0013639049369634877, 'epoch': 1.07}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Checkpoint destination directory bnalyv/tinyllama\\checkpoint-90 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.6236, 'grad_norm': 0.3221951425075531, 'learning_rate': 0.0012089675630312753, 'epoch': 1.2}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Checkpoint destination directory bnalyv/tinyllama\\checkpoint-100 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.6333, 'grad_norm': 0.37463831901550293, 'learning_rate': 0.0010485622221144484, 'epoch': 1.33}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Checkpoint destination directory bnalyv/tinyllama\\checkpoint-110 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.5849, 'grad_norm': 0.36563369631767273, 'learning_rate': 0.0008868861738047158, 'epoch': 1.47}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Checkpoint destination directory bnalyv/tinyllama\\checkpoint-120 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.5607, 'grad_norm': 0.37835443019866943, 'learning_rate': 0.0007281699277636571, 'epoch': 1.6}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Checkpoint destination directory bnalyv/tinyllama\\checkpoint-130 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.6775, 'grad_norm': 0.37425386905670166, 'learning_rate': 0.0005765665457425102, 'epoch': 1.73}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Checkpoint destination directory bnalyv/tinyllama\\checkpoint-140 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.6229, 'grad_norm': 0.3600066006183624, 'learning_rate': 0.0004360429701490934, 'epoch': 1.87}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Checkpoint destination directory bnalyv/tinyllama\\checkpoint-150 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.6042, 'grad_norm': 0.5924780964851379, 'learning_rate': 0.00031027622272189573, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Checkpoint destination directory bnalyv/tinyllama\\checkpoint-160 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.5181, 'grad_norm': 0.39385610818862915, 'learning_rate': 0.0002025571894372794, 'epoch': 2.13}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Checkpoint destination directory bnalyv/tinyllama\\checkpoint-170 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.4489, 'grad_norm': 0.48114246129989624, 'learning_rate': 0.00011570450926997656, 'epoch': 2.27}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Checkpoint destination directory bnalyv/tinyllama\\checkpoint-180 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.3336, 'grad_norm': 0.4597238600254059, 'learning_rate': 5.199082004372957e-05, 'epoch': 2.4}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Checkpoint destination directory bnalyv/tinyllama\\checkpoint-190 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.2814, 'grad_norm': 0.4304306209087372, 'learning_rate': 1.3083291266109298e-05, 'epoch': 2.53}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Checkpoint destination directory bnalyv/tinyllama\\checkpoint-200 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.384, 'grad_norm': 0.4029560387134552, 'learning_rate': 0.0, 'epoch': 2.67}\n",
      "{'train_runtime': 460.198, 'train_samples_per_second': 2.608, 'train_steps_per_second': 0.435, 'train_loss': 2.6900500679016113, 'epoch': 2.67}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=200, training_loss=2.6900500679016113, metrics={'train_runtime': 460.198, 'train_samples_per_second': 2.608, 'train_steps_per_second': 0.435, 'train_loss': 2.6900500679016113, 'epoch': 2.67})"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Training\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\bbnfa\\Python_bgdn\\LLaMA\\.venv\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unk or sober Ive got no use for words\n",
      "Its all gone wrong and Im left with nothing but pain\n",
      "And thats what Im feeling\n",
      " \n",
      "Screaming into my face\n",
      "My eyes are open wide shut\n",
      "The blood is pouring down my nose\n",
      "Theres something that needs killing\n",
      "It wont let go until im dead\n",
      " \n",
      "Blood everywhere\n",
      "All over the place\n",
      "Living inside your head\n",
      "You cant escape the fact that life has been taken away\n",
      "No matter how much you try\n",
      "Your body can never get used to being alive\n",
      "So long as there is breath\n",
      "Then Ill take it like a maniac\n",
      " \n",
      "Walking through the streets\n",
      "Ain't gonna give up\n",
      "Just because I am strong\n",
      "That doesn't mean I have to die\n",
      " \n",
      "Stupid boy who thought he was cool\n",
      "He thinks his name is Rage\n",
      "Rage\n",
      " \n",
      "Damned if I dont kill him\n",
      "Cause I know better than to trust myself\n",
      "Into my mind I knew better than to believe\n",
      "Now I feel like Im crazy\n",
      " \n",
      "Painting my face red\n",
      "With blood coming from my ears\n",
      "Tears streaming\n"
     ]
    }
   ],
   "source": [
    "# Generating lyrics with fine-tuned model\n",
    "text_output_ft = generate_lyrics(test_data[2100:2600], model)\n",
    "\n",
    "print(text_output_ft.replace(\",\", \"\\n\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
